# 优化版近邻搜索算法比较

本文档介绍了优化版的近邻搜索算法比较程序，针对原始main.py运行速度慢的问题进行了多项优化。

## 优化策略

1. **高效数据读取**：
   - 使用`np.fromstring`替代逐元素解析
   - 使用`float32`代替默认的`float64`减少内存使用
   - 预分配数组避免动态增长

2. **内存使用优化**：
   - 使用`psutil`直接测量内存使用而非`tracemalloc`
   - 避免频繁开启和关闭内存追踪

3. **并行处理**：
   - 使用多进程处理查询点
   - 自动适应CPU核心数量
   - 针对小数据集自动降级为单进程处理

4. **进度追踪**：
   - 显示每个文件的处理时间
   - 定期保存中间结果（每10个文件）
   - 输出总体运行时间

5. **图表优化**：
   - 绘图后立即关闭图形（减少内存占用）
   - 智能跳过不适用的图表（例如，当所有维度相同时）

## 使用方法

```bash
python optimized_main.py [options]
```

### 命令行选项

- `--data_dir`: 数据文件目录（默认: ./data）
- `--max_files`: 处理的最大文件数量（默认: 100）
- `--max_queries`: 每个文件处理的最大查询点数（默认: 100）
- `--bbf_t`: BBF搜索访问的最大叶节点数（默认: 200）
- `--no_multiprocessing`: 禁用多进程处理

### 示例

处理前10个文件，每个文件50个查询点:
```bash
python optimized_main.py --max_files 10 --max_queries 50
```

禁用多进程处理:
```bash
python optimized_main.py --no_multiprocessing
```

## 性能对比

原始版本 vs 优化版本性能预期提升：

| 方面 | 原始版本 | 优化版本 | 提升幅度 |
|------|---------|---------|---------|
| 数据读取 | 逐元素解析 | 批量读取 | ~2-3倍 |
| 内存跟踪 | tracemalloc | psutil | ~1.5倍 |
| 并行处理 | 单线程 | 多进程 | 与CPU核心数相关，约2-4倍 |
| 总体性能 | 20+秒/文件 | 预期5-10秒/文件 | ~3倍 |

## 中间结果保存

为防止长时间运行中断导致数据丢失，每处理10个文件会保存一次中间结果:
- `results/summary_results_temp_10.csv`
- `results/summary_results_temp_20.csv`
- ...等

## 注意事项

1. 多进程模式下，使用的是共享内存模型，内存使用会略微增加
2. 处理大型数据集时，建议确保系统有足够的内存空间
3. 如果程序占用过多内存，可以考虑减小`max_queries`参数值 